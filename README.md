# ASLVideoTranslate

American Sign Language (ASL) video translation system using V-JEPA video embeddings and transformer-based classification.

## Overview

ASLVideoTranslate is a deep learning project that:
- Encodes ASL videos using **V-JEPA2** (Vision-based Joint Embedding Predictive Architecture) to extract spatiotemporal features
- Trains a **GlossClassifier** to map video embeddings to ASL glosses (signs)
- Provides real-time ASL translation via a **Streamlit web interface**
- Supports inference on live webcam feeds

## Features

- **V-JEPA2 Video Encoding**: Pre-trained video foundation model for robust feature extraction
- **Attention-based Classification**: Temporal attention mechanism over video frame embeddings
- **Real-time Inference**: Live ASL recognition from webcam
- **Multi-variant Processing**: Supports original, Gaussian-blurred, and B&W video variants
- **Robust Data Pipeline**: Video preprocessing with frame sampling and augmentation

## Installation

### Requirements
- Python 3.9+
- CUDA-capable GPU (recommended)
- FFmpeg (for video processing)

### Setup

1. Clone the repository:
```bash
git clone https://github.com/yourusername/ASLVideoTranslate.git
cd ASLVideoTranslate
```

2. Create a virtual environment:
```bash
python3 -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
pip install streamlit transformers huggingface_hub decord opencv-python pillow tqdm scikit-learn
```

## Project Structure

```
├── src/
│   ├── train.py                    # Model training script
│   ├── process_videos.py           # Video encoding and preprocessing
│   ├── dataset.py                  # Dataset class for loading embeddings
│   ├── vjepa_encoder.py            # V-JEPA2 encoder wrapper
│   ├── streamlit_demo.py           # Web UI for real-time translation
│   ├── embedding_to_asl_gloss.py  # Utility functions
│   ├── models/
│   │   └── asl_classifier.py      # GlossClassifier architecture
│   └── test.py                     # Testing utilities
├── data/
│   ├── WLASL_v0.3.json            # WLASL dataset metadata
│   ├── top_glosses.txt            # Selected ASL sign vocabulary
│   ├── selected_videos/           # Processed embeddings
│   ├── videos/                    # Raw ASL videos
│   └── processed_videos/          # V-JEPA embeddings (numpy files)
├── models/                         # Saved model checkpoints
├── requirements.txt
└── README.md
```

## Usage

### 1. Process Videos and Extract Embeddings

Convert raw ASL videos to V-JEPA embeddings:

```bash
cd src
python process_videos.py
```

This will:
- Load videos from `data/videos/`
- Extract V-JEPA2 features per frame
- Generate 3 variants: original, Gaussian blur, B&W
- Save `.npy` embeddings to `data/selected_videos/`
- Create `index.csv` for dataset indexing

### 2. Train the GlossClassifier

Train the model on extracted embeddings:

```bash
python train.py --batch 32 --epochs 10 --lr 1e-5
```

**Arguments:**
- `--batch`: Batch size (default: 8)
- `--epochs`: Number of training epochs (default: 1)
- `--lr`: Learning rate (default: 1e-3)

The model will be saved to `models/gloss_classifier_<timestamp>.pt`

### 3. Run Real-time ASL Translation

Launch the Streamlit web interface:

```bash
streamlit run streamlit_demo.py
```

Open http://localhost:8501 in your browser to:
- Stream from webcam
- View real-time ASL gloss predictions
- See temporal attention heatmaps
- Adjust confidence thresholds

## Model Architecture

### GlossClassifier

```
Input: [B, T, D] (B=batch, T=frames, D=1408)
  ↓
Attention Module: Linear(1408 → 1) → Softmax over T
  ↓
Weighted Aggregation: [B, D]
  ↓
Classification Network:
  - Linear(1408 → 512) + LayerNorm + GELU
  - Linear(512 → 256) + LayerNorm + GELU
  - Linear(256 → num_classes)
  ↓
Output: [B, num_classes] logits
```

## Data Format

### Video Embeddings
- Format: `.npy` numpy arrays
- Shape: `(T, 1408)` where T is sequence length
- Dtype: float32/bfloat16
- Generated by V-JEPA2 model

### Index File (index.csv)
```csv
video_id,gloss,path_to_npy_file
good_25076_gaussian_vjepa.npy,good,/path/to/good_25076_gaussian_vjepa.npy
...
```

## Key Components

### VjepaDataset
Custom PyTorch Dataset class that:
- Loads pre-computed V-JEPA embeddings
- Maps glosses to class indices
- Handles variable-length sequences

### VJEPA2Encoder
Wrapper around HuggingFace V-JEPA2 model:
- Preprocesses video to 256×256 @ 16fps
- Applies normalization (ImageNet stats)
- Extracts patch-wise features from last layer

### LiveASLTranslator
Real-time inference engine:
- Buffers frames into sliding windows
- Aggregates predictions over time
- Maintains attention weights for visualization

## Training Tips

- **GPU**: Requires CUDA-capable GPU (11GB+ VRAM recommended)
- **Dtype**: Code uses `float16` for memory efficiency
- **Batch Size**: Larger batches (32-64) recommended with GPU
- **Learning Rate**: Use 1e-5 to 1e-4 for fine-tuning
- **Data Order**: Dataset uses stratified train/val split (80/20)

## Troubleshooting

### CUDA Capability Error
```
RuntimeError: mat1 and mat2 must have the same dtype
```
Solution: Model and inputs must match dtypes. Code uses `float16` by default.

### Out of Memory
Reduce batch size: `python train.py --batch 8`

### Corrupted Video Files
The dataset automatically validates `.npy` files and skips corrupted entries.

## Dataset (WLASL)

This project uses the **WLASL (World Level American Sign Language)** dataset:
- ~2,000 ASL signs (glosses)
- 22,500+ video samples
- Publicly available: https://github.com/dxli94/WLASL

To use your own WLASL data:
1. Download from the repository
2. Extract videos to `data/videos/`
3. Place `WLASL_v0.3.json` in `data/`
4. Run `process_videos.py`

## Performance

Typical metrics on validation set:
- **Accuracy**: ~70-80% (top-1) on 300 glosses
- **Inference Speed**: ~5-10 FPS real-time on GPU
- **Model Size**: ~5-10MB (classifier only)

## Future Work

- [ ] Sentence-level translation with sequence models
- [ ] Multi-modal fusion (pose + hand tracking)
- [ ] Continuous sign recognition without frame boundaries
- [ ] Transfer learning from larger vision models
- [ ] Mobile deployment with ONNX/TFLite

## References

- [V-JEPA: Video Joint Embedding Predictive Architecture](https://arxiv.org/abs/2212.04957)
- [WLASL Dataset](https://github.com/dxli94/WLASL)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)

## License

MIT License

## Contributing

Contributions welcome! Please open issues and pull requests for bugs/features.

## Contact

For questions or collaboration, please reach out.